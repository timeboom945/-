## 最小二乘法与伪逆
用直线拟合数据集$D=\{(\bold{x_1},y_1)\cdots (\bold{x_n},y_N)\}$，其中$\bold{x}$是向量
驾驶向量$\bold{x}$是$p$维的，$\bold{x}\in \R^p，y\in\R，i=1,2,\cdots,N$
就是用p个维度的数据去拟合y
定义
$$
\bold{X}=(\bold{x_1},\cdots,\bold{x_N})^T=\left(\begin{matrix}
x_{11}&x_{12}&x_{13}&\cdots&x_{1p}\\
&&\vdots\\
x_{N1}&x_{N2}&x_{N3}&\cdots&x_{Np}
\end{matrix}\right)_{N\times p}\\
\bold{Y}=\left(\begin{matrix}
y_1\\
y_2\\
\vdots\\
y_N
\end{matrix}\right)_{N\times 1}
$$
现在要求**系数向量$\bold{w}$**
$$
\bold{w}=\left(\begin{matrix}
a_1\\
a_2\\
\vdots\\
a_p
\end{matrix}\right)
$$
使得
$$
\begin{aligned}
\bold{w}^T\left(\begin{matrix}
\bold{x_1},\bold{x_2},\dots,\bold{x_N}
\end{matrix}\right)&=\bold{w}^T\left(\begin{matrix}
x_{11}&x_{21}&\dots&x_{N1}\\
&&\vdots\\
x_{1p}&x_{2p}&\dots&x_{Np}
\end{matrix}\right)\\
&=(\hat y_1,\hat y_2,\dots,\hat y_N)=\bold{\hat y}^T\tag 1
\end{aligned}
$$
最接近真实的向量$\bold{y}$
所以定义损失函数
$$
L(\bold{w})=\sum_1^N\Vert \bold{w}^T\bold{x_i}-y_i\Vert^2=\sum_1^N\Vert \hat y_i-y_i\Vert^2
$$
来表示估计的偏差
用$\bold{X}$和$\bold{Y}$来表示损失函数
$$
\begin{aligned}
L(\bold{w})&=\sum_1^N\Vert \bold{w}^T\bold{x_i}-y_i\Vert^2\\
&=\sum_1^N(\bold{w}^T\bold{x_i}-y_i)^2\\
&=\left(\begin{matrix}
\bold{w}^T\bold{x_1}-y_1,\bold{w}^T\bold{x_2}-y_2,\dots,\bold{w}^T\bold{x_N}-y_N
\end{matrix}\right)\left(\begin{matrix}
\bold{w}^T\bold{x_1}-y_1\\
\bold{w}^T\bold{x_2}-y_2\\
\vdots\\
\bold{w}^T\bold{x_N}-y_N
\end{matrix}\right)\\
&=\Big(\left(\begin{matrix}
\bold{w}^T\bold{x_1},\bold{w}^T\bold{x_1},\dots,\bold{w}^T\bold{x_1}
\end{matrix}\right)-\left(\begin{matrix}
y_1,y_2,\dots,y_N
\end{matrix}\right)\Big)\left(\begin{matrix}
\bold{w}^T\bold{x_1}-y_1\\
\bold{w}^T\bold{x_2}-y_2\\
\vdots\\
\bold{w}^T\bold{x_N}-y_N
\end{matrix}\right)\\
&=\Big(\bold{w}^T\bold{X}^T-\bold{Y}^T\Big)\Big(\bold{X}\bold{w}-\bold{Y}\Big)\\
&=\bold{w^TX^TXw}-\underbrace{2\bold{w^TX^TY}}_{\bold{w^TX^TY和Y^TXw互为转置又都是常数}}+\bold{Y^TY}\\
\end{aligned}
$$
要使损失函数最小，就对系数向量进行求导使其为0向量
$$
\dfrac{\partial L(\bold{w})}{\partial \bold{w}}=\bold{0}
$$
___
### 矩阵求导
#### 标量方程对向量的导数
两种表现形式
1. 分母布局
假设$f(y_1,y_2)=y_1^2+y_2^2$，向量$\bold{y}=\left(\begin{matrix}
y_1\\
y_2
\end{matrix}\right)$
那么
$$
\dfrac{\partial f(\bold{y})}{\partial \bold{y}}=\left(\begin{matrix}
\dfrac{\partial f}{\partial y_1}\\
\\
\dfrac{\partial f}{\partial y_2}
\end{matrix}\right)
$$
2. 分子布局
$$
\dfrac{\partial f(\bold{y})}{\partial \bold{y}}=\left(\begin{matrix}
\dfrac{\partial f}{\partial y_1}\\
\\
\dfrac{\partial f}{\partial y_2}
\end{matrix}\right)^T=\left(\begin{matrix}
\dfrac{\partial f}{\partial y_1},\dfrac{\partial f}{\partial y_2}
\end{matrix}\right)
$$

分母布局和分子布局仅表现形式上不同，互为转置
下面默认分母布局
#### 向量方程对向量的导数
设向量函数(返回的是一个向量)$\bold{f(y)}=\left(\begin{matrix}
f_1(\bold{y})\\
f_2(\bold{y})\\
\vdots\\
f_n(\bold{y})\\
\end{matrix}\right)$
比如$\bold{f(y)}=(y_1+y_2)\bold{i}+(y_1y_2)\bold{j}+(y_1-y_2)\bold{k}=\left(\begin{matrix}
y_1+y_2\\
y_1y_2\\
y_1-y_2
\end{matrix}\right)$
那么
$$
\dfrac{\partial \bold{f(y)}}{\partial \bold{y}}=\left(\begin{matrix}
\dfrac{\partial \bold{f}}{\partial y_1}\\
\\
\dfrac{\partial \bold{f}}{\partial y_2}\\
\end{matrix}\right)
=\left(\begin{matrix}
\dfrac{\partial f_1}{\partial y_1},\dfrac{\partial f_2}{\partial y_1},\dfrac{\partial f_3}{\partial y_1}\\
\\
\dfrac{\partial f_1}{\partial y_2},\dfrac{\partial f_2}{\partial y_2},\dfrac{\partial f_3}{\partial y_2}
\end{matrix}\right)_{2\times 3}
$$
#### 重要的结论
若$\bold{y}=\left(\begin{matrix}
y_1\\
y_2\\
\vdots\\
y_m
\end{matrix}\right)$，$A=\left(\begin{matrix}
a_{11},\dots,a_{1m}\\
\vdots\\
a_{m1},\dots,a_{mm}
\end{matrix}\right)$
那么
$$
\dfrac{\partial \bold{y^T}A\bold{y}}{\partial \bold{y}}=A\bold{y}+A^T\bold{y}\tag 2
$$
并且，当$A$对称时
$$
\dfrac{\partial \bold{y^T}A\bold{y}}{\partial \bold{y}}=2A\bold{y}\tag 3
$$
___
故损失函数的导数
$$
\begin{aligned}
\dfrac{\partial L(\bold{w})}{\partial \bold{w}}&=\dfrac{\partial \bold{w^TX^TXw}}{\partial \bold{w}}-2\dfrac{\partial \bold{w^TX^TY}}{\partial \bold{w}}\\
&=2\bold{X^TXw}-2\bold{X^TY}\\
&=\bold{0}
\end{aligned}
$$
因此
$$
\bold{X^TXw}=\bold{X^TY}\\
\bold{w}=\bold{\underbrace{(X^TX)^{-1}X^T}_{伪逆}Y}
$$
此时损失函数最小
### 几何意义
$\bold{X_{N\times p}}$($N>>P$)在一个$p$维上张成了一个空间，每个样本点都在这个空间内，
?
## 概率视角看线性回归
$D=\{(\bold{x_1},y_1)\cdots (\bold{x_n},y_N)\}$
$$
\bold{X}=(\bold{x_1},\cdots,\bold{x_N})^T=\left(\begin{matrix}
x_{11}&x_{12}&x_{13}&\cdots&x_{1p}\\
&&\vdots\\
x_{N1}&x_{N2}&x_{N3}&\cdots&x_{Np}
\end{matrix}\right)_{N\times p}\\
\bold{Y}=\left(\begin{matrix}
y_1\\
y_2\\
\vdots\\
y_N
\end{matrix}\right)_{N\times 1}
$$
损失函数
$$
L(\bold{w})=\sum_1^N\Vert \bold{w}^T\bold{x_i}-y_i\Vert^2
$$
系数向量的最好估计为
$$
\bold{\hat w}=argmin_{\bold{w}}L(\bold{w})=\bold{(X^TX)^{-1}X^TY}
$$
现在假设噪音是正态的
$$
\epsilon\sim N(0,\sigma^2)\\
y=f(\bold{w})+\epsilon=\hat y+\epsilon
$$
**这实际上就是制定了寻找拟合$y$的线的标准：要找到一条用来预测$y$的线，使得真实的值$y$和预测的值$\hat y$的差为$\epsilon$**
因为
$$
\epsilon\sim N(0,\sigma^2)\\
$$
所以
$$
\overbrace{y\vert \bold{w}}^{\bold{x}是讨论背景不是随机变量}\sim N(\hat y,\sigma^2)
$$
___
$$
\bold{u}=\left(\begin{matrix}
u_1\\
u_2\\
\vdots\\
u_p
\end{matrix}\right)，那么\\
\begin{aligned}
&p(\bold{u})=p(\left(\begin{matrix}
u_1\\
u_2\\
\vdots\\
u_p
\end{matrix}\right))=p(u_1,u_2,\dots,u_p)\tag 4\\
&p(\bold{u})=p(u_1\vert \left(\begin{matrix}
u_2\\
u_3\\
\vdots\\
u_p
\end{matrix}\right))p(u_2\vert \left(\begin{matrix}
u_3\\
u_4\\
\vdots\\
u_p
\end{matrix}\right))\dots p(u_{p-1}\vert u_p)p(u_p)\\
&p(\bold{u})=p(u_1)p(u_2\vert u_1)p(u_3\vert \left(\begin{matrix}
u_1\\
u_2\\
\end{matrix}\right))\dots p(u_{p-1}\vert \left(\begin{matrix}
u_1\\
u_2\\
\vdots\\
u_{p-2}
\end{matrix}\right))p(u_p\vert \left(\begin{matrix}
u_1\\
u_2\\
\vdots\\
u_{p-1}
\end{matrix}\right))
\end{aligned}
$$
并且当$\bold{u}$各项独立时有
$$
p(\bold{u})=\prod_1^pp(u_i)
$$
___
其极大似然损失函数为
$$
L(\bold{w})=logP(\bold{y}\vert \bold{w})=log\overbrace{\prod_1^NP(y_i\vert \bold{w})}^{独立性}=\sum_1^NlogP(y_i\vert \bold{w})\\
=\sum_1^N\left(log\dfrac{1}{\sqrt{2\pi}\sigma}-\dfrac{1}{2\sigma^2}(y_i-\bold{w^T}\bold{x_i})^2\right)
$$
要使得$L(\bold{w})$最大，那么就要使$\sum_1^N\dfrac{1}{2\sigma^2}(y_i-\bold{w^T}\bold{x_i})^2最小$，因此就和最小二乘法的损失函数一致
**最小二乘法*设法让*噪音服从正态分布**
## 正则化
由损失函数得到的
$$
\bold{\hat w} =\bold{(X^TX)^{-1}X^TY}
$$
其中$\bold{(X^TX)}$有可能不可逆，比如样本点过少，因此很容易**过拟合**(有用的信息对比噪音占不到上风，过度学习了噪音)
对付过拟合的方法有
1. 加数据
2. 降维(减少噪音的学习)
3. 正则化(设立惩罚)

**正则化标准框架**
正则化的损失函数
$$
L_\lambda(\bold{w})=argmin_{\bold{w}}[L(\bold{w})+\lambda P(\bold{w})]
$$
1. lasso
   $P(\bold{w})=\Vert \bold{w}\Vert=\sqrt{a_1^2+\dots+a_p^2}$
2. ridge(岭回归)
   $P(\bold{w})=\Vert \bold{w}\Vert^2=\bold{w^Tw}=a_1^2+\dots+a_p^2$
   权值衰减

### 岭回归
加入正则化后的损失函数要使其最小，就有
$$
\dfrac{\partial L_\lambda{\bold{w}}}{\partial \bold{w}}=\bold{0}
$$
于是
$$
\begin{aligned}
\dfrac{\partial L_\lambda{\bold{w}}}{\partial \bold{w}}&=\dfrac{\partial L{\bold{w}}}{\partial \bold{w}}+\dfrac{\partial \lambda\bold{w^Tw}}{\partial \bold{w}}\\
&=2\bold{X^TXw}-2\bold{X^TY}+2\lambda\bold{w}
\end{aligned}
$$
于是有
$$
\bold{X^TXw}-\bold{X^TY}+\lambda\bold{w}=0
$$
所以
$$
\bold{\hat w_\lambda}=\overbrace{(\bold{\underbrace{X^TX}_{半正定矩阵}+\lambda I})^{-1}}^{一定可逆了，其特征值都大>0}\bold{X^TY}
$$
### 贝叶斯视角的岭回归
同样的
$$
y=\hat y+\epsilon\\
\epsilon\sim N(0,\sigma^2)\\
y\vert \bold{w}\sim N(\bold{w^Tx}=\hat y,\sigma^2)
$$
那么
$$
\overbrace{p(y\vert \bold{w})}^{要注意\bold{x}不是变量，是背景}=\dfrac{1}{\sqrt{2\pi}\sigma}e^{\dfrac{(y-\hat y)^2}{-2\sigma^2}}\\
$$
给$\bold{w}$一个共轭先验
$$
\bold{w}\sim N_p(\bold{0},\Lambda)，\Lambda=\left(\begin{matrix}
\sigma_0^2&&&&&\bold{0}\\
&\sigma_0^2&&&&\\
&&\sigma_0^2&&&\\
&&&\ddots&&\\
&&&&\sigma_0^2&\\
\bold{0}&&&&&\sigma_0^2\\
\end{matrix}\right)
$$
$\bold{w}$各项独立，所以关系系数$\rho_{ij}=0$
因此
$$
\begin{aligned}
p(\bold{w})&=\prod_1^pp(w_i)\\
&=\prod_1^p\dfrac{1}{\sqrt{2\pi}\sigma_0}e^{\dfrac{(w_i-0)^2}{-2\sigma^2_0}}\\
&=\dfrac{1}{(\sqrt{2\pi}\sigma_0)^p}e^{\dfrac{w_1^2}{-2\sigma^2_0}+\dfrac{w_2^2}{-2\sigma^2_0}+\dots+\dfrac{w_p^2}{-2\sigma^2_0}}\\
&=\dfrac{1}{(\sqrt{2\pi}\sigma_0)^p}e^{\dfrac{\Vert \bold{w}\Vert^2}{-2\sigma^2_0}}
\end{aligned}
$$
___
如果向量$\bold{w}$的每个分量$w_i$都服从
$$
w_i\sim N(0,\sigma^2_0)
$$
那么向量$\bold{w}$服从
$$
\bold{w}\sim N_p(\bold{0},\Lambda)，\Lambda=\left(\begin{matrix}
\sigma_0^2&&&&&\bold{0}\\
&\sigma_0^2&&&&\\
&&\sigma_0^2&&&\\
&&&\ddots&&\\
&&&&\sigma_0^2&\\
\bold{0}&&&&&\sigma_0^2\\
\end{matrix}\right)_{p\times p}
$$
并且有
$$
p(\bold{w})=\dfrac{1}{(\sqrt{2\pi}\sigma_0)^p}e^{\dfrac{\Vert \bold{w}\Vert^2}{-2\sigma^2_0}}\tag 5
$$
___
所以由贝叶斯定理
$$
p(\bold{w}\vert y)p(y)=p(y\vert \bold{w})p(\bold{w})
$$
要求出向量$\bold{w}$使得$p(\bold{w}\vert y)$最大，因此有贝叶斯$MAP$
$$
\begin{aligned}
MAP&=argmax_{\bold{w}}\ln[p(y\vert \bold{w})p(\bold{w})]\\
&=argmax_{\bold{w}}\ln\left(\dfrac{1}{\sqrt{2\pi}\sigma}e^{\dfrac{(y-\hat y)^2}{-2\sigma^2}}\right)\left(\dfrac{1}{(\sqrt{2\pi}\sigma_0)^p}e^{\dfrac{\Vert \bold{w}\Vert^2}{-2\sigma^2_0}}\right)\\
&=argmax_{\bold{w}}\left\{\ln\left(\dfrac{1}{\sqrt{2\pi}\sigma}\dfrac{1}{(\sqrt{2\pi}\sigma_0)^p}\right)+\ln e^{\dfrac{(y-\hat y)^2}{-2\sigma^2}+\dfrac{\Vert \bold{w}\Vert^2}{-2\sigma^2_0}}\right\}\\
&=argmax_{\bold{w}}\left\{\dfrac{(y-\hat y)^2}{-2\sigma^2}+\dfrac{\Vert \bold{w}\Vert^2}{-2\sigma^2_0}\right\}\\
&=argmin_{\bold{w}}\left\{\dfrac{(y-\hat y)^2}{\sigma^2}+\dfrac{\Vert \bold{w}\Vert^2}{\sigma^2_0}\right\}\\
&=argmin_{\bold{w}}L(\bold{w})+argmin_{\bold{w}}\dfrac{\sigma^2}{\sigma^2_0}\Vert \bold{w}\Vert^2\\
&=argmin_{\bold{w}}L_{\lambda=\frac{\sigma^2}{\sigma^2_0}}(\bold{w})\tag 6
\end{aligned}
$$
**所以岭回归实际是贝叶斯给予噪音$\epsilon$和$\bold{w}$先验的最大后验估计**