## 概率生成模型
$$
\{(\bold{x_i},y_i)\}_{i=1}^N，\bold{x_i}\in\R^p，y_i\in \{0,1\}\\
y取0或者是1
$$
判定模型通过激活函数直接计算$p(y\vert \bold{x_i})$
思想是尽可能学会其中的**规律**而不是学习其中的知识，让判别出来的概率分布和真实的概率分布尽可能相似
概率生成模型关心联合概率分布$p(y,\bold{x_i})$，需要给出$y$的先验概率
$$
p(y\vert \bold{x_i})\propto p(\bold{x_i}\vert y)p(y)\\
\hat y = argmax_{y\in\{0,1\}}\log p(y\vert \bold{x_i})=argmax_{y\in\{0,1\}}\log p(y)p(\bold{x_i}\vert y)
$$
给与$y$一个先验分布
这个分布可以是伯努利分布
$$
y\sim Bernoulli(\phi)\\
P(y=1)=\phi，P(y=0)=1-\phi\\
$$
## 高斯判别分析过程
假设方差矩阵$\sum$是相同的
$$
\bold{x_i}\vert y=1\sim N(\bold{\mu_1},\sum)\\
\bold{x_i}\vert y=0\sim N(\bold{\mu_0},\sum)\\
$$
并且先验$p(y)$服从伯努利分布
$$
p(y)=\phi^y(1-\phi)^{1-y}，y\in\{0,1\}
$$

和判别分析先学习判别规律$\bold{w}$一样，高斯判别分析先学习**知识**
这个知识包括
1. 先验伯努利
$$
p(y)的\phi
$$
2. 不同类别的样本的规律
$$
\bold{x_i}\vert y=1\sim N(\bold{\mu_1},\sum)\\
\bold{x_i}\vert y=0\sim N(\bold{\mu_0},\sum)\\
$$
中的期望向量$\mu_1$和$\mu_0$和方差矩阵$\sum$

最后通过学习到的知识$(\mu_0,\mu_1,\sum,\phi)$来判断新的样本$\bold{x_{new}}$的类别$y_{new}$，来让$y_{new}$使联合概率密度$p(\bold{x_{new}},y_{new})$最大化
### 知识学习
$\theta$取最大后验
$$
\bold{\theta}=(\mu_1,\mu_0,\sum,\phi)^T,\mu_1,\mu_0\in\R^p\\
\hat \theta=argmax_{\theta}L(\theta)\\
\begin{aligned}
L(\bold{\theta})&=\log \prod_{i=1}^{N}p(\bold{x_i},y_i)，y_i\in \{0,1\}\\
&=\sum_1^N\log p(\bold{x_i}\vert y_i)p(y_i)\\
&=\sum_1^N\log p(\bold{x_i}\vert y_i)+\sum_1^N\log p(y_i)\\
&=\sum_1^N\log N(\mu_1,\sum)^{y_i}N(\mu_0,\sum)^{1-y_i}+\sum_1^N\log \phi^{y_i}(1-\phi)^{1-y_i}\\
&=\sum_1^N\left\{y_i\log N(\mu_1,\sum)+(1-y_i)\log N(\mu_0,\sum)+\log \phi^{y_i}(1-\phi)^{1-y_i}\right\}
\end{aligned}
$$
1. 先学习$\phi$
   $$
   \begin{aligned}
    \dfrac{\partial \sum_1^N\log \phi^{y_i}(1-\phi)^{1-y_i}}{\partial \phi}&=\dfrac{\partial \sum_1^Ny_i\log \phi+\sum_1^N(1-y_i)\log(1-\phi)}{\partial \phi}\\
    &=\sum_1^Ny_i\dfrac{1}{\phi}-\sum_1^N(1-y_i)\dfrac{1}{1-\phi}\\
    &=0
    \end{aligned}
   $$
就有
$$
\sum_1^N\dfrac{y_i}{\phi}=\sum_1^N\dfrac{1-y_i}{1-\phi}
$$
所以
$$
(1-\phi)\sum_1^Ny_i=\phi\sum_1^N(1-y_i)=\phi\left(N-\sum_1^Ny_i\right)
$$
因此
$$
\phi = \dfrac{\sum_1^Ny_i}{N}=\bar y
$$
2. 再学习$\mu_1$和$\mu_0$
   $\mu_1$和$\mu_0$的求法是一样的
   $$
   \begin{aligned}
    \dfrac{\partial \sum_1^Ny_i\log N(\mu_1,\sum)}{\partial \mu_1}&=\dfrac{\partial \sum_1^Ny_i\log \dfrac{1}{(2\pi)^{\dfrac{p}{2}}\vert \sum \vert^{\dfrac{1}{2}}}e^{-\dfrac{1}{2}\bold{(x_i-\mu_1)^T\sum^{-1}(x_i-\mu_1)}}}{\partial \mu_1}\\
    &=\dfrac{\partial -\dfrac{1}{2}\sum_1^Ny_i\bold{(x_i-\mu_1)^T\sum^{-1}(x_i-\mu_1)}}{\partial \mu_1}\\
    &=-\dfrac{1}{2}\dfrac{\partial \sum_1^Ny_i\left(\bold{x^T_i\sum^{-1}-\mu_1^T\sum^{-1}}\right)\bold{(x_i-\mu_1)}}{\partial \mu_1}\\
    &=-\dfrac{1}{2}\dfrac{\partial \sum_1^Ny_i\left(\bold{x^T_i\sum^{-1}x_i-2\mu_1^T\sum^{-1}x_i+\mu_1^T\sum^{-1}\mu_1}\right)}{\partial \mu_1}\\
    &=-\dfrac{1}{2}\sum_1^Ny_i(-2)\bold{{\sum}^{-1}x_i}-\dfrac{1}{2}\sum_1^Ny_i2\bold{{\sum}^{-1}\mu_1}\\
    &=\sum_1^Ny_i\left(\bold{{\sum}^{-1}x_i-{\sum}^{-1}\mu_1}\right)\\
    &=\bold{0}
    \end{aligned}
   $$
   那么就有
   $$
    \sum_1^Ny_i\bold{x_i}=\sum_1^Ny_i\bold{\mu_1}
   $$
   所以
   $$
    \mu_1=\dfrac{\sum_1^Ny_i\bold{x_i}}{\sum_1^Ny_i}=\dfrac{\sum_1^Ny_i\bold{x_i}}{N_1}
   $$
   其中$N_1$是$y=1$样本的个数
   同理
   $$
    \mu_0=\dfrac{\sum_1^N(1-y_i)\bold{x_i}}{N_0}
   $$
3. 最后学习$\sum$
   $$
    L(\theta)=\sum_1^N\left\{y_i\log N(\mu_1,\sum)+(1-y_i)\log N(\mu_0,\sum)+\log \phi^{y_i}(1-\phi)^{1-y_i}\right\}
   $$
   那么
   $$
    \begin{aligned}
    \dfrac{\partial L(\theta)}{\partial \sum}&=\dfrac{\partial \sum_{y_i\in N_1}\log N(\mu_1,\sum)+\sum_{y_i\in N_0}\log N(\mu_0,\sum)}{\partial \sum}\\
    \end{aligned}
   $$
   有
   $$
   \begin{aligned}
    \log N(\mu_1,\sum)&=\log \dfrac{1}{(2\pi)^{\dfrac{p}{2}}}\dfrac{1}{\vert \sum\vert^{\dfrac{1}{2}}}e^{-\dfrac{1}{2}\bold{(x_i-\mu_1)^T\sum^{-1}(x_i-\mu_1)}}\\
    &=\log \dfrac{1}{(2\pi)^{\dfrac{p}{2}}}+\log\dfrac{1}{\vert \sum\vert^{\dfrac{1}{2}}}-\dfrac{1}{2}\bold{(x_i-\mu_1)^T{\sum}^{-1}(x_i-\mu_1)}
    \end{aligned}
   $$
   因此
   $$
   \begin{aligned}
   \dfrac{\partial L(\theta)}{\partial \sum}&=\dfrac{\sum_{y_i\in N_1}\left\{\log\dfrac{1}{\vert \sum\vert^{\dfrac{1}{2}}}-\dfrac{1}{2}\bold{(x_i-\mu_1)^T{\sum}^{-1}(x_i-\mu_1)}
   \right\}}{\partial \sum}+\\
   &\dfrac{\sum_{y_i\in N_0}\left\{\log\dfrac{1}{\vert \sum\vert^{\dfrac{1}{2}}}-\dfrac{1}{2}\bold{(x_i-\mu_0)^T{\sum}^{-1}(x_i-\mu_0)}
   \right\}}{\partial \sum}
   \end{aligned}
   $$
___
### 矩阵的迹与矩阵求导
$$
tr(AB)=tr(BA)\\
tr(ABC)=tr(CBA)=tr(BCA)\\
tr(A)+tr(B)=tr(A+B)
$$
矩阵求导
$$
\dfrac{\partial tr(AB)}{\partial A}=B^T\\
\dfrac{\partial \vert A\vert}{\partial A}=\vert A\vert A^{-1}
$$
___
马氏距离$\bold{(x_i-\mu_1)^T{\sum}^{-1}(x_i-\mu_1)}$的迹$tr[\bold{(x_i-\mu_1)^T{\sum}^{-1}(x_i-\mu_1)}]$有
$$
\begin{aligned}
tr[\bold{(x_i-\mu_1)^T{\sum}^{-1}(x_i-\mu_1)}]&=tr[(x_i-\mu_1)(x_i-\mu_1)^T{\sum}^{-1}]\\
&=(x_i-\mu_1)^T{\sum}^{-1}(x_i-\mu_1)
\end{aligned}
$$
因此
$$
\begin{aligned}
&\sum_{y_i\in N_1}\left\{\log\dfrac{1}{\vert \sum\vert^{\dfrac{1}{2}}}-\dfrac{1}{2}\bold{(x_i-\mu_1)^T{\sum}^{-1}(x_i-\mu_1)}\right\}\\
&=-\dfrac{1}{2}\sum_{y_i\in N_1}\log \vert \sum\vert-\dfrac{1}{2}\sum_{y_i\in N_1}tr[(x_i-\mu_1)^T{\sum}^{-1}(x_i-\mu_1)]\\
&=-\dfrac{1}{2}\sum_{y_i\in N_1}\log \vert \sum\vert-\dfrac{1}{2}\sum_{y_i\in N_1}tr[(x_i-\mu_1)(x_i-\mu_1)^T{\sum}^{-1}]\\
&=-\dfrac{1}{2}\sum_{y_i\in N_1}\log \vert \sum\vert-\dfrac{1}{2}tr\left[\sum_{y_i\in N_1}(x_i-\mu_1)(x_i-\mu_1)^T{\sum}^{-1}\right]\\
&=-\dfrac{1}{2}N_1\log \vert \sum\vert-\dfrac{1}{2}N_1tr\left[S_{y_i\in N_1}{\sum}^{-1}\right]
\end{aligned}
$$
所以
$$
\begin{aligned}
\dfrac{\partial L(\theta)}{\partial \sum}&=\dfrac{-\dfrac{1}{2}N_1\log \vert \sum\vert-\dfrac{1}{2}N_1tr\left[S_{y_i\in N_1}{\sum}^{-1}\right]}{\partial \sum}+\\
&\dfrac{-\dfrac{1}{2}N_0\log \vert \sum\vert-\dfrac{1}{2}N_0tr\left[S_{y_i\in N_0}{\sum}^{-1}\right]}{\partial \sum}\\
&=\dfrac{-\dfrac{1}{2}N\log \vert \sum\vert-\dfrac{1}{2}N_1tr\left[S_{y_i\in N_1}{\sum}^{-1}\right]-\dfrac{1}{2}N_0tr\left[S_{y_i\in N_0}{\sum}^{-1}\right]}{\partial \sum}\\
&=-\dfrac{1}{2}N\dfrac{1}{\vert \sum\vert}\vert \sum\vert{\sum}^{-1}+\dfrac{1}{2}N_1S_{y_i\in N_1}{\sum}^{-2}+\dfrac{1}{2}N_0S_{y_i\in N_0}{\sum}^{-2}\\
&=-\dfrac{1}{2}{\sum}^{-1}+\dfrac{1}{2}N_1S_{y_i\in N_1}{\sum}^{-2}+\dfrac{1}{2}N_0S_{y_i\in N_0}{\sum}^{-2}\\
&=\bold{0}_{p\times p}
\end{aligned}
$$
___
$$
\begin{aligned}
\dfrac{\partial tr(S\sum^{-1})}{\partial \sum}&=\dfrac{\partial tr(\sum^{-1}S)}{\partial \sum}\\
&=\dfrac{\partial tr(\sum^{-1}S)}{\partial \sum^{-1}}\dfrac{\partial \sum^{-1}}{\partial \sum}\\
&=S^T(-1)\dfrac{1}{\sum^2}\\
&=-S^T{\sum}^{-2}
\end{aligned}
$$
___
因此有
$$
{\sum}^{-1}+N_1S_{y_i\in N_1}{\sum}^{-2}+N_0S_{y_i\in N_0}{\sum}^{-2}=\bold{0}\\
\sum+N_1S_{y_i\in N_1}+N_0S_{y_i\in N_0}=\bold{0}
$$
所以
$$
\hat \sum=-\left(N_1S_{y_i\in N_1}+N_0S_{y_i\in N_0}\right)
$$
## 总结
$$
\phi = \dfrac{\sum_1^Ny_i}{N}=\bar y\\
\mu_1=\dfrac{\sum_1^Ny_i\bold{x_i}}{\sum_1^Ny_i}=\dfrac{\sum_1^Ny_i\bold{x_i}}{N_1}\\
\mu_0=\dfrac{\sum_1^N(1-y_i)\bold{x_i}}{N_0}\\
\hat \sum=-\left(N_1S_{y_i\in N_1}+N_0S_{y_i\in N_0}\right)
$$
其中
$$
S_{y_i\in N_1}=\dfrac{1}{N_1}\sum_{y_i\in N_1}(\bold{(x_i-\mu_1)(x_i-\mu_1)^T})\\
S_{y_i\in N_0}=\dfrac{1}{N_0}\sum_{y_i\in N_0}(\bold{(x_i-\mu_0)(x_i-\mu_0)^T})
$$